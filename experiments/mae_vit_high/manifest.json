{
  "files": [
    {
      "path": "config.yaml",
      "content": "seed: 42\ndevice: \"cuda\"\noutput_dir: \"checkpoints\"\n\nmodel:\n  img_size: 64\n  patch_size: 8\n  in_chans: 3\n  embed_dim: 256\n  depth: 6\n  num_heads: 8\n  mlp_ratio: 4.0\n  decoder_embed_dim: 128\n  decoder_depth: 4\n  decoder_num_heads: 4\n  mask_ratio: 0.75\n\ntraining:\n  batch_size: 64\n  epochs: 10\n  lr: 1.0e-3\n  weight_decay: 0.0\n  num_workers: 2\n  print_freq: 10\n  resume: false\n  checkpoint_path: \"checkpoints/mae_vit.pth\"\n  eval_interval: 1\n\ndataset:\n  name: \"synthetic\"\n  image_size: 64\n  in_chans: 3\n  train_size: 2048\n  val_size: 256\n  image_folder_path: \"\"\n"
    },
    {
      "path": "model.py",
      "content": "import torch\nfrom torch import nn\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n        self.norm2 = nn.LayerNorm(dim)\n        hidden_dim = int(dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, dim),\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        y = self.norm1(x)\n        attn_output, _ = self.attn(y, y, y, need_weights=False)\n        x = x + self.dropout(attn_output)\n        y = self.norm2(x)\n        y = self.mlp(y)\n        x = x + self.dropout(y)\n        return x\n\n\nclass MAEViT(nn.Module):\n    def __init__(\n        self,\n        img_size=64,\n        patch_size=8,\n        in_chans=3,\n        embed_dim=256,\n        depth=6,\n        num_heads=8,\n        mlp_ratio=4.0,\n        decoder_embed_dim=128,\n        decoder_depth=4,\n        decoder_num_heads=4,\n        mask_ratio=0.75,\n    ):\n        super().__init__()\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.mask_ratio = mask_ratio\n\n        assert img_size % patch_size == 0, \"img_size must be divisible by patch_size\"\n        self.num_patches = (img_size // patch_size) ** 2\n\n        # Patch embedding\n        self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n        # Positional embedding for encoder\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n\n        # Encoder\n        self.encoder_blocks = nn.ModuleList(\n            [TransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)]\n        )\n        self.encoder_norm = nn.LayerNorm(embed_dim)\n\n        # Decoder\n        if decoder_embed_dim != embed_dim:\n            self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim)\n        else:\n            self.decoder_embed = nn.Identity()\n\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, decoder_embed_dim))\n\n        self.decoder_blocks = nn.ModuleList(\n            [TransformerBlock(decoder_embed_dim, decoder_num_heads, mlp_ratio) for _ in range(decoder_depth)]\n        )\n        self.decoder_norm = nn.LayerNorm(decoder_embed_dim)\n\n        patch_dim = patch_size * patch_size * in_chans\n        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_dim)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        nn.init.normal_(self.pos_embed, std=0.02)\n        nn.init.normal_(self.decoder_pos_embed, std=0.02)\n        nn.init.normal_(self.mask_token, std=0.02)\n\n        def _init(m):\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n        self.apply(_init)\n\n    def random_masking(self, x, mask_ratio):\n        \"\"\"Per-sample random masking.\n\n        x: [B, N, C]\n        Returns: x_masked, mask, ids_restore\n        \"\"\"\n        B, N, C = x.shape\n        len_keep = int(N * (1 - mask_ratio))\n\n        noise = torch.rand(B, N, device=x.device)\n        ids_shuffle = torch.argsort(noise, dim=1)\n        ids_restore = torch.argsort(ids_shuffle, dim=1)\n\n        ids_keep = ids_shuffle[:, :len_keep]\n        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, C))\n\n        mask = torch.ones(B, N, device=x.device)\n        mask[:, :len_keep] = 0\n        mask = torch.gather(mask, dim=1, index=ids_restore)\n\n        return x_masked, mask, ids_restore\n\n    def forward_encoder(self, x):\n        # x: [B, C, H, W]\n        x = self.patch_embed(x)  # [B, embed_dim, H/ps, W/ps]\n        x = x.flatten(2).transpose(1, 2)  # [B, N, embed_dim]\n\n        x = x + self.pos_embed\n        x, mask, ids_restore = self.random_masking(x, self.mask_ratio)\n\n        for blk in self.encoder_blocks:\n            x = blk(x)\n        x = self.encoder_norm(x)\n        return x, mask, ids_restore\n\n    def forward_decoder(self, x, ids_restore):\n        x = self.decoder_embed(x)\n        B, N_visible, C = x.shape\n        N = ids_restore.shape[1]\n        num_mask = N - N_visible\n\n        mask_tokens = self.mask_token.repeat(B, num_mask, 1)\n        x_ = torch.cat([x, mask_tokens], dim=1)\n\n        ids_restore_expanded = ids_restore.unsqueeze(-1).repeat(1, 1, C)\n        x_ = torch.gather(x_, dim=1, index=ids_restore_expanded)\n\n        x_ = x_ + self.decoder_pos_embed\n\n        for blk in self.decoder_blocks:\n            x_ = blk(x_)\n        x_ = self.decoder_norm(x_)\n        x_ = self.decoder_pred(x_)\n        return x_\n\n    def forward(self, imgs):\n        latent, mask, ids_restore = self.forward_encoder(imgs)\n        pred = self.forward_decoder(latent, ids_restore)\n        return pred, mask\n"
    },
    {
      "path": "dataset.py",
      "content": "import torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import datasets, transforms\n\n\nclass SyntheticImagesDataset(Dataset):\n    def __init__(self, length=10000, image_size=64, in_chans=3, seed=0):\n        self.length = length\n        self.image_size = image_size\n        self.in_chans = in_chans\n        self.seed = seed\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        generator = torch.Generator()\n        generator.manual_seed(self.seed + idx)\n        img = torch.rand(self.in_chans, self.image_size, self.image_size, generator=generator)\n        return img\n\n\ndef build_dataloaders(cfg):\n    ds_cfg = cfg[\"dataset\"]\n    tr_cfg = cfg[\"training\"]\n    name = ds_cfg.get(\"name\", \"synthetic\").lower()\n    seed = cfg.get(\"seed\", 42)\n\n    if name == \"synthetic\":\n        train_dataset = SyntheticImagesDataset(\n            length=ds_cfg.get(\"train_size\", 10000),\n            image_size=ds_cfg.get(\"image_size\", 64),\n            in_chans=ds_cfg.get(\"in_chans\", 3),\n            seed=seed,\n        )\n        val_size = ds_cfg.get(\"val_size\", 1000)\n        if val_size and val_size > 0:\n            val_dataset = SyntheticImagesDataset(\n                length=val_size,\n                image_size=ds_cfg.get(\"image_size\", 64),\n                in_chans=ds_cfg.get(\"in_chans\", 3),\n                seed=seed + 100000,\n            )\n        else:\n            val_dataset = None\n\n    elif name == \"imagefolder\":\n        root = ds_cfg.get(\"image_folder_path\", \"\")\n        if not root:\n            raise ValueError(\"dataset.image_folder_path must be set for imagefolder dataset.\")\n\n        img_size = ds_cfg.get(\"image_size\", 64)\n        transform = transforms.Compose(\n            [\n                transforms.Resize(img_size),\n                transforms.CenterCrop(img_size),\n                transforms.ToTensor(),\n            ]\n        )\n\n        full_dataset = datasets.ImageFolder(root=root, transform=transform)\n        if len(full_dataset) == 0:\n            raise ValueError(f\"No images found in {root}\")\n\n        val_size = max(1, int(0.1 * len(full_dataset)))\n        train_size = len(full_dataset) - val_size\n        generator = torch.Generator()\n        generator.manual_seed(seed)\n        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n\n    else:\n        raise ValueError(f\"Unknown dataset name: {ds_cfg['name']}\")\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=tr_cfg.get(\"batch_size\", 64),\n        shuffle=True,\n        num_workers=tr_cfg.get(\"num_workers\", 2),\n        pin_memory=True,\n    )\n\n    if val_dataset is not None and len(val_dataset) > 0:\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=tr_cfg.get(\"batch_size\", 64),\n            shuffle=False,\n            num_workers=tr_cfg.get(\"num_workers\", 2),\n            pin_memory=True,\n        )\n    else:\n        val_loader = None\n\n    return train_loader, val_loader\n"
    },
    {
      "path": "train.py",
      "content": "import argparse\nimport os\n\nimport torch\nfrom torch import optim\n\nfrom model import MAEViT\nfrom dataset import build_dataloaders\nfrom losses import mae_loss\nimport utils\nimport metrics\n\n\ndef train_one_epoch(model, data_loader, optimizer, device, cfg, epoch):\n    model.train()\n    running_loss = 0.0\n    num_samples = 0\n    print_freq = cfg[\"training\"].get(\"print_freq\", 10)\n    patch_size = cfg[\"model\"][\"patch_size\"]\n\n    for step, batch in enumerate(data_loader):\n        if isinstance(batch, (list, tuple)):\n            imgs = batch[0]\n        else:\n            imgs = batch\n\n        imgs = imgs.to(device, non_blocking=True)\n\n        optimizer.zero_grad()\n        pred, mask = model(imgs)\n        target = utils.patchify(imgs, patch_size)\n        loss = mae_loss(pred, target, mask)\n        loss.backward()\n        optimizer.step()\n\n        batch_size = imgs.size(0)\n        running_loss += loss.item() * batch_size\n        num_samples += batch_size\n\n        if (step + 1) % print_freq == 0:\n            avg_loss = running_loss / max(num_samples, 1)\n            print(\n                f\"Epoch [{epoch + 1}] Step [{step + 1}/{len(data_loader)}] \"\n                f\"Train Loss: {avg_loss:.4f}\"\n            )\n\n    epoch_loss = running_loss / max(num_samples, 1)\n    return epoch_loss\n\n\ndef evaluate(model, data_loader, device, cfg):\n    if data_loader is None:\n        return None, None\n\n    model.eval()\n    patch_size = cfg[\"model\"][\"patch_size\"]\n    val_loss = 0.0\n    mse_total = 0.0\n    num_samples = 0\n\n    with torch.no_grad():\n        for batch in data_loader:\n            if isinstance(batch, (list, tuple)):\n                imgs = batch[0]\n            else:\n                imgs = batch\n\n            imgs = imgs.to(device, non_blocking=True)\n\n            pred, mask = model(imgs)\n            target = utils.patchify(imgs, patch_size)\n            loss = mae_loss(pred, target, mask)\n\n            batch_size = imgs.size(0)\n            val_loss += loss.item() * batch_size\n            mse_total += metrics.patch_mse(pred, target) * batch_size\n            num_samples += batch_size\n\n    avg_loss = val_loss / max(num_samples, 1)\n    avg_mse = mse_total / max(num_samples, 1)\n    return avg_loss, avg_mse\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"MAE ViT Self-Supervised Training\")\n    parser.add_argument(\"--config\", type=str, default=\"config.yaml\", help=\"Path to config file\")\n    parser.add_argument(\n        \"overrides\",\n        nargs=\"*\",\n        help=\"Configuration overrides in key=value format\",\n        default=None,\n    )\n    args = parser.parse_args()\n\n    cfg = utils.load_config(args.config, args.overrides)\n    seed = cfg.get(\"seed\", 42)\n    utils.set_seed(seed)\n\n    requested_device = cfg.get(\"device\", \"auto\")\n    if requested_device == \"auto\":\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif requested_device == \"cuda\" and not torch.cuda.is_available():\n        print(\"CUDA requested but not available. Falling back to CPU.\")\n        device = torch.device(\"cpu\")\n    else:\n        device = torch.device(requested_device)\n\n    print(f\"Using device: {device}\")\n\n    train_loader, val_loader = build_dataloaders(cfg)\n\n    model = MAEViT(**cfg[\"model\"]).to(device)\n\n    optimizer = optim.Adam(\n        model.parameters(),\n        lr=cfg[\"training\"].get(\"lr\", 1e-3),\n        weight_decay=cfg[\"training\"].get(\"weight_decay\", 0.0),\n    )\n\n    start_epoch = 0\n    num_epochs = cfg[\"training\"].get(\"epochs\", 10)\n    checkpoint_path = cfg[\"training\"].get(\"checkpoint_path\", \"checkpoints/mae_vit.pth\")\n    resume = cfg[\"training\"].get(\"resume\", False)\n\n    if resume and os.path.isfile(checkpoint_path):\n        print(f\"Loading checkpoint from {checkpoint_path}\")\n        checkpoint = utils.load_checkpoint(checkpoint_path, map_location=device)\n        model.load_state_dict(checkpoint[\"model\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n        start_epoch = checkpoint.get(\"epoch\", 0) + 1\n        print(f\"Resumed from epoch {start_epoch}\")\n    elif resume:\n        print(f\"Checkpoint {checkpoint_path} not found. Starting from scratch.\")\n\n    eval_interval = cfg[\"training\"].get(\"eval_interval\", 1)\n\n    for epoch in range(start_epoch, num_epochs):\n        train_loss = train_one_epoch(model, train_loader, optimizer, device, cfg, epoch)\n\n        val_loss = None\n        val_mse = None\n        if val_loader is not None and ((epoch + 1) % eval_interval == 0):\n            val_loss, val_mse = evaluate(model, val_loader, device, cfg)\n\n        if val_loss is not None:\n            print(\n                f\"Epoch [{epoch + 1}/{num_epochs}] \"\n                f\"Train Loss: {train_loss:.4f} \"\n                f\"Val Loss (masked): {val_loss:.4f} \"\n                f\"Val MSE (unmasked): {val_mse:.4f}\"\n            )\n        else:\n            print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {train_loss:.4f}\")\n\n        state = {\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n            \"epoch\": epoch,\n            \"config\": cfg,\n        }\n        utils.save_checkpoint(state, checkpoint_path)\n\n    print(\"Training finished.\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "path": "losses.py",
      "content": "import torch\n\n\ndef mae_loss(pred, target, mask):\n    \"\"\"Mean squared error over masked patches.\n\n    pred: [B, N, P]\n    target: [B, N, P]\n    mask: [B, N], 1 for masked, 0 for visible\n    \"\"\"\n    # Per-patch MSE\n    loss = (pred - target) ** 2\n    loss = loss.mean(dim=-1)\n\n    if mask.sum() == 0:\n        return loss.mean()\n\n    loss = (loss * mask).sum() / mask.sum()\n    return loss\n"
    },
    {
      "path": "metrics.py",
      "content": "import torch\n\n\ndef patch_mse(pred, target):\n    \"\"\"Mean squared error over all patches and patch elements.\"\"\"\n    return torch.mean((pred - target) ** 2).item()\n\n\ndef patch_mae(pred, target):\n    \"\"\"Mean absolute error over all patches and patch elements.\"\"\"\n    return torch.mean(torch.abs(pred - target)).item()\n"
    },
    {
      "path": "utils.py",
      "content": "import os\nimport random\nimport math\n\nimport numpy as np\nimport torch\nimport yaml\n\n\ndef set_seed(seed):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef parse_value(raw):\n    lower = raw.lower()\n    if lower in (\"true\", \"false\"):\n        return lower == \"true\"\n    try:\n        if \".\" in raw:\n            return float(raw)\n        return int(raw)\n    except ValueError:\n        return raw\n\n\ndef apply_overrides(cfg, overrides):\n    for ov in overrides or []:\n        if \"=\" not in ov:\n            continue\n        key_str, value_str = ov.split(\"=\", 1)\n        keys = key_str.split(\".\")\n        d = cfg\n        for k in keys[:-1]:\n            if k not in d or not isinstance(d[k], dict):\n                d[k] = {}\n            d = d[k]\n        d[keys[-1]] = parse_value(value_str)\n    return cfg\n\n\ndef load_config(path, overrides=None):\n    with open(path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n    cfg = apply_overrides(cfg, overrides)\n    return cfg\n\n\ndef save_checkpoint(state, path):\n    directory = os.path.dirname(path)\n    if directory:\n        os.makedirs(directory, exist_ok=True)\n    torch.save(state, path)\n\n\ndef load_checkpoint(path, map_location=None):\n    return torch.load(path, map_location=map_location)\n\n\ndef patchify(imgs, patch_size):\n    \"\"\"Convert images to patch sequences.\n\n    imgs: [B, C, H, W]\n    returns: [B, N, patch_size*patch_size*C]\n    \"\"\"\n    B, C, H, W = imgs.shape\n    assert H % patch_size == 0 and W % patch_size == 0, \"Image dimensions must be divisible by patch_size\"\n    h = H // patch_size\n    w = W // patch_size\n\n    x = imgs.reshape(B, C, h, patch_size, w, patch_size)\n    x = x.permute(0, 2, 4, 3, 5, 1)  # [B, h, w, p, p, C]\n    x = x.reshape(B, h * w, patch_size * patch_size * C)\n    return x\n\n\ndef unpatchify(patches, patch_size, channels):\n    \"\"\"Convert patch sequences back to images.\n\n    patches: [B, N, patch_size*patch_size*C]\n    returns: [B, C, H, W]\n    \"\"\"\n    B, N, D = patches.shape\n    h = w = int(math.sqrt(N))\n    assert h * w == N, \"Number of patches must be a perfect square\"\n\n    x = patches.reshape(B, h, w, patch_size, patch_size, channels)\n    x = x.permute(0, 5, 1, 3, 2, 4)  # [B, C, h, p, w, p]\n    x = x.reshape(B, channels, h * patch_size, w * patch_size)\n    return x\n"
    },
    {
      "path": "README.md",
      "content": "# MAE ViT Self-Supervised Training\n\nThis repository implements a minimal Masked Autoencoder (MAE) with a Vision Transformer (ViT) backbone using pure PyTorch.\n\nFeatures:\n- MAE ViT encoder/decoder with patchify + random masking\n- Self-supervised reconstruction objective (masked patch MSE)\n- Synthetic dataset option (no external downloads)\n- Simple train + eval loops\n- Config-driven hyperparameters via `config.yaml` with CLI overrides\n- Checkpoint save/load\n- Seeded and deterministic behavior (as far as PyTorch allows)\n\n## Installation\n\nCreate a virtual environment (optional) and install dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Configuration\n\nAll main options live in `config.yaml`:\n\n- `model.*`: ViT/MAE architecture (image size, patch size, dimensions, depths, heads, mask ratio)\n- `training.*`: batch size, epochs, learning rate, workers, checkpoint path, etc.\n- `dataset.*`:\n  - `name`: `\"synthetic\"` (default) or `\"imagefolder\"`\n  - `image_size`, `in_chans`: image shape\n  - `train_size`, `val_size`: sizes for synthetic dataset\n  - `image_folder_path`: root directory for `ImageFolder` when using real images\n\nDefaults are set up to run on a small synthetic dataset for quick sanity checks.\n\n## Running Training\n\nWith defaults from `config.yaml`:\n\n```bash\npython train.py --config config.yaml\n```\n\nTraining logs per-epoch metrics to stdout and saves checkpoints to the path defined in `training.checkpoint_path`.\n\n### CLI Overrides\n\nYou can override any config value from the command line using `key=value` pairs (dot notation for nested keys):\n\n```bash\npython train.py --config config.yaml \\\n    training.epochs=20 \\\n    training.batch_size=128 \\\n    model.mask_ratio=0.9\n```\n\nExample using a real image folder:\n\n```bash\npython train.py --config config.yaml \\\n    dataset.name=imagefolder \\\n    dataset.image_folder_path=/path/to/images \\\n    dataset.image_size=128\n```\n\n## Checkpointing\n\n- The latest checkpoint is saved after every epoch to `training.checkpoint_path`.\n- To resume from that checkpoint, set `training.resume: true` in `config.yaml` or override it:\n\n```bash\npython train.py --config config.yaml training.resume=true\n```\n\nIf the checkpoint file does not exist, training will start from scratch.\n\n## Reproducibility\n\n`utils.set_seed` is called at startup with `seed` from `config.yaml` to initialize:\n- Python `random`\n- NumPy\n- PyTorch CPU and CUDA RNGs\n- cuDNN determinism settings\n\nNote: Some operations in PyTorch may still introduce nondeterminism depending on your hardware and version.\n\n## Files Overview\n\n- `config.yaml` \u2013 default configuration\n- `model.py` \u2013 MAE ViT encoder/decoder\n- `dataset.py` \u2013 synthetic dataset and DataLoader builder (optionally `ImageFolder`)\n- `train.py` \u2013 main training & evaluation loop\n- `losses.py` \u2013 masked reconstruction loss\n- `metrics.py` \u2013 simple reconstruction metrics\n- `utils.py` \u2013 config loading, seeding, checkpointing, patchify helpers\n- `requirements.txt` \u2013 Python dependencies\n"
    },
    {
      "path": "requirements.txt",
      "content": "torch\ntorchvision\nPyYAML\nnumpy\n"
    }
  ]
}