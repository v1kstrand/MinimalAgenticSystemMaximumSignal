{
  "files": [
    {
      "path": "config.yaml",
      "content": "seed: 42\noutput_dir: checkpoints\n\nmodel:\n  image_size: 32\n  patch_size: 4\n  in_channels: 3\n  embed_dim: 256\n  depth: 4\n  num_heads: 4\n  mlp_ratio: 4.0\n  decoder_embed_dim: 128\n  decoder_depth: 2\n  decoder_num_heads: 4\n  mask_ratio: 0.75\n\ntraining:\n  epochs: 10\n  batch_size: 64\n  lr: 0.001\n  weight_decay: 0.05\n  device: cuda\n  num_workers: 2\n  log_interval: 100\n  resume_path: null\n\ndataset:\n  name: synthetic\n  train_size: 10000\n  val_size: 2000\n  image_size: 32\n  in_channels: 3\n"
    },
    {
      "path": "model.py",
      "content": "import math\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn\n\n\nclass PatchEmbed(nn.Module):\n    def __init__(self, img_size: int, patch_size: int, in_chans: int, embed_dim: int) -> None:\n        super().__init__()\n        assert img_size % patch_size == 0, 'Image size must be divisible by patch size.'\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x: (B, C, H, W)\n        x = self.proj(x)  # (B, D, H/P, W/P)\n        x = x.flatten(2).transpose(1, 2)  # (B, N, D)\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4.0, drop: float = 0.0) -> None:\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n        self.norm2 = nn.LayerNorm(dim)\n\n        hidden_dim = int(dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, dim),\n        )\n        self.drop = nn.Dropout(drop) if drop > 0 else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Self-attention\n        x_attn = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n        x = x + self.drop(x_attn)\n        # MLP\n        x_mlp = self.mlp(self.norm2(x))\n        x = x + self.drop(x_mlp)\n        return x\n\n\nclass MAEViT(nn.Module):\n    def __init__(\n        self,\n        img_size: int = 32,\n        patch_size: int = 4,\n        in_chans: int = 3,\n        embed_dim: int = 256,\n        depth: int = 4,\n        num_heads: int = 4,\n        mlp_ratio: float = 4.0,\n        decoder_embed_dim: int = 128,\n        decoder_depth: int = 2,\n        decoder_num_heads: int = 4,\n        mask_ratio: float = 0.75,\n    ) -> None:\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.mask_ratio = mask_ratio\n\n        # Encoder\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n        self.encoder_blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)\n        ])\n        self.encoder_norm = nn.LayerNorm(embed_dim)\n\n        # Decoder\n        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim)\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches, decoder_embed_dim))\n        self.decoder_blocks = nn.ModuleList([\n            TransformerBlock(decoder_embed_dim, decoder_num_heads, mlp_ratio) for _ in range(decoder_depth)\n        ])\n        self.decoder_norm = nn.LayerNorm(decoder_embed_dim)\n        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size * patch_size * in_chans)\n\n        self._init_weights()\n\n    def _init_weights(self) -> None:\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.zeros_(m.bias)\n                nn.init.ones_(m.weight)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.decoder_pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.mask_token, std=0.02)\n\n    def patchify(self, imgs: torch.Tensor) -> torch.Tensor:\n        # imgs: (B, C, H, W)\n        p = self.patch_size\n        B, C, H, W = imgs.shape\n        assert H == self.img_size and W == self.img_size\n        x = imgs.reshape(B, C, H // p, p, W // p, p)\n        x = x.permute(0, 2, 4, 3, 5, 1)  # (B, H/P, W/P, p, p, C)\n        x = x.reshape(B, -1, p * p * C)   # (B, N, P^2*C)\n        return x\n\n    def unpatchify(self, x: torch.Tensor) -> torch.Tensor:\n        # x: (B, N, P^2*C)\n        p = self.patch_size\n        B, N, D = x.shape\n        C = self.in_chans\n        h = w = int(math.sqrt(N))\n        assert h * w == N\n        x = x.reshape(B, h, w, p, p, C)\n        x = x.permute(0, 5, 1, 3, 2, 4)  # (B, C, H, W)\n        imgs = x.reshape(B, C, h * p, w * p)\n        return imgs\n\n    def random_masking(self, x: torch.Tensor, mask_ratio: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        # x: (B, N, D)\n        B, N, D = x.shape\n        len_keep = int(N * (1 - mask_ratio))\n\n        noise = torch.rand(B, N, device=x.device)\n        ids_shuffle = torch.argsort(noise, dim=1)\n        ids_restore = torch.argsort(ids_shuffle, dim=1)\n\n        ids_keep = ids_shuffle[:, :len_keep]\n        x_masked = torch.gather(x, 1, ids_keep.unsqueeze(-1).repeat(1, 1, D))\n\n        mask = torch.ones(B, N, device=x.device)\n        mask[:, :len_keep] = 0\n        mask = torch.gather(mask, 1, ids_restore)\n\n        return x_masked, mask, ids_restore\n\n    def forward(self, imgs: torch.Tensor, mask_ratio: float = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        if mask_ratio is None:\n            mask_ratio = self.mask_ratio\n\n        # Patch embedding\n        x = self.patch_embed(imgs)\n        x = x + self.pos_embed\n\n        # Masking\n        x_masked, mask, ids_restore = self.random_masking(x, mask_ratio)\n\n        # Encoder\n        for blk in self.encoder_blocks:\n            x_masked = blk(x_masked)\n        x_masked = self.encoder_norm(x_masked)\n\n        # Decoder\n        x_dec = self.decoder_embed(x_masked)\n        B, N_vis, D_dec = x_dec.shape\n        num_patches = self.patch_embed.num_patches\n        num_mask = num_patches - N_vis\n\n        mask_tokens = self.mask_token.repeat(B, num_mask, 1)\n        x_ = torch.cat([x_dec, mask_tokens], dim=1)\n        x_ = torch.gather(\n            x_,\n            1,\n            ids_restore.unsqueeze(-1).repeat(1, 1, D_dec),\n        )\n        x_ = x_ + self.decoder_pos_embed\n\n        for blk in self.decoder_blocks:\n            x_ = blk(x_)\n        x_ = self.decoder_norm(x_)\n\n        pred = self.decoder_pred(x_)  # (B, N, P^2*C)\n        target = self.patchify(imgs)\n        return pred, target, mask\n"
    },
    {
      "path": "dataset.py",
      "content": "import random\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass SyntheticImageDataset(Dataset):\n    def __init__(self, size: int, image_size: int, in_channels: int, seed: int = 0) -> None:\n        super().__init__()\n        self.size = size\n        self.image_size = image_size\n        self.in_channels = in_channels\n\n        g = torch.Generator()\n        g.manual_seed(seed)\n        self.data = torch.rand(size, in_channels, image_size, image_size, generator=g)\n\n    def __len__(self) -> int:\n        return self.size\n\n    def __getitem__(self, idx: int) -> torch.Tensor:\n        return self.data[idx]\n\n\ndef _seed_worker(worker_id: int, base_seed: int) -> None:\n    worker_seed = base_seed + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\n\ndef get_dataloader(config: Dict, train: bool = True) -> DataLoader:\n    ds_cfg = config['dataset']\n    name = ds_cfg['name']\n    if name != 'synthetic':\n        raise ValueError(f'Unsupported dataset name: {name}. Only \"synthetic\" is implemented.')\n\n    size_key = 'train_size' if train else 'val_size'\n    size = int(ds_cfg[size_key])\n    image_size = int(ds_cfg['image_size'])\n    in_channels = int(ds_cfg['in_channels'])\n\n    seed = int(config.get('seed', 0)) + (0 if train else 1)\n    dataset = SyntheticImageDataset(size, image_size, in_channels, seed=seed)\n\n    batch_size = int(config['training']['batch_size'])\n    num_workers = int(config['training']['num_workers'])\n\n    generator = torch.Generator()\n    generator.manual_seed(seed)\n\n    def worker_init_fn(worker_id: int) -> None:\n        _seed_worker(worker_id, seed)\n\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=train,\n        num_workers=num_workers,\n        pin_memory=True,\n        worker_init_fn=worker_init_fn,\n        generator=generator,\n    )\n    return loader\n"
    },
    {
      "path": "losses.py",
      "content": "from typing import Tuple\n\nimport torch\n\n\ndef mae_loss(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"Mean-squared error on masked patches only.\n\n    pred, target: (B, N, D)\n    mask: (B, N) where 1 indicates masked patches\n    \"\"\"\n    B, N, D = pred.shape\n    mask = mask.unsqueeze(-1)  # (B, N, 1)\n\n    mse = (pred - target) ** 2\n    mse = mse.mean(dim=-1, keepdim=True)  # (B, N, 1)\n\n    masked_mse = (mse * mask).sum() / mask.sum().clamp(min=1.0)\n    return masked_mse\n"
    },
    {
      "path": "metrics.py",
      "content": "from typing import Optional\n\nimport torch\n\n\ndef reconstruction_mse(pred: torch.Tensor, target: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n    \"\"\"Average per-patch MSE; if mask is given, only over masked patches.\"\"\"\n    mse = (pred - target) ** 2\n    mse = mse.mean(dim=-1)  # (B, N)\n    if mask is not None:\n        mse = (mse * mask).sum() / mask.sum().clamp(min=1.0)\n    else:\n        mse = mse.mean()\n    return mse\n"
    },
    {
      "path": "utils.py",
      "content": "import argparse\nimport os\nimport random\nfrom typing import Any, Dict, List\n\nimport numpy as np\nimport torch\nimport yaml\n\n\ndef seed_everything(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef ensure_dir(path: str) -> None:\n    os.makedirs(path, exist_ok=True)\n\n\ndef save_checkpoint(state: Dict[str, Any], path: str) -> None:\n    ensure_dir(os.path.dirname(path) or '.')\n    torch.save(state, path)\n\n\ndef load_checkpoint(path: str, model: torch.nn.Module, optimizer: torch.optim.Optimizer = None, map_location: str = 'cpu') -> Dict[str, Any]:\n    checkpoint = torch.load(path, map_location=map_location)\n    model.load_state_dict(checkpoint['model_state'])\n    if optimizer is not None and 'optimizer_state' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state'])\n    return checkpoint\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description='MAE ViT self-supervised training')\n    parser.add_argument('--config', type=str, default='config.yaml', help='Path to config file')\n    parser.add_argument('overrides', nargs='*', help='Configuration overrides in key=value format')\n    return parser.parse_args()\n\n\ndef _set_in_dict(d: Dict[str, Any], keys: List[str], value: Any) -> None:\n    cur = d\n    for k in keys[:-1]:\n        if k not in cur or not isinstance(cur[k], dict):\n            cur[k] = {}\n        cur = cur[k]\n    cur[keys[-1]] = value\n\n\ndef apply_overrides(config: Dict[str, Any], overrides: List[str]) -> Dict[str, Any]:\n    for ov in overrides:\n        if '=' not in ov:\n            continue\n        key, val = ov.split('=', 1)\n        key = key.strip()\n        val = val.strip()\n        if not key:\n            continue\n        try:\n            parsed_val = yaml.safe_load(val)\n        except Exception:\n            parsed_val = val\n        _set_in_dict(config, key.split('.'), parsed_val)\n    return config\n\n\ndef load_config(path: str, overrides: List[str]) -> Dict[str, Any]:\n    with open(path, 'r') as f:\n        cfg = yaml.safe_load(f) or {}\n    cfg = apply_overrides(cfg, overrides)\n    return cfg\n"
    },
    {
      "path": "train.py",
      "content": "import os\nimport time\nfrom typing import Dict\n\nimport torch\nfrom torch import optim\n\nfrom dataset import get_dataloader\nfrom losses import mae_loss\nfrom metrics import reconstruction_mse\nfrom model import MAEViT\nfrom utils import (\n    ensure_dir,\n    load_checkpoint,\n    load_config,\n    parse_args,\n    save_checkpoint,\n    seed_everything,\n)\n\n\ndef build_model(config: Dict) -> MAEViT:\n    mcfg = config['model']\n    model = MAEViT(\n        img_size=int(mcfg['image_size']),\n        patch_size=int(mcfg['patch_size']),\n        in_chans=int(mcfg['in_channels']),\n        embed_dim=int(mcfg['embed_dim']),\n        depth=int(mcfg['depth']),\n        num_heads=int(mcfg['num_heads']),\n        mlp_ratio=float(mcfg.get('mlp_ratio', 4.0)),\n        decoder_embed_dim=int(mcfg['decoder_embed_dim']),\n        decoder_depth=int(mcfg['decoder_depth']),\n        decoder_num_heads=int(mcfg.get('decoder_num_heads', mcfg['num_heads'])),\n        mask_ratio=float(mcfg['mask_ratio']),\n    )\n    return model\n\n\ndef get_device(config: Dict) -> torch.device:\n    cfg_device = str(config['training']['device'])\n    if cfg_device.startswith('cuda') and not torch.cuda.is_available():\n        print('CUDA not available, falling back to CPU.')\n        return torch.device('cpu')\n    return torch.device(cfg_device)\n\n\ndef train_one_epoch(model: MAEViT, loader, optimizer, device, config: Dict, epoch: int) -> float:\n    model.train()\n    running_loss = 0.0\n    total_samples = 0\n    log_interval = int(config['training'].get('log_interval', 100))\n    mask_ratio = float(config['model']['mask_ratio'])\n\n    for step, imgs in enumerate(loader):\n        imgs = imgs.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        pred, target, mask = model(imgs, mask_ratio=mask_ratio)\n        loss = mae_loss(pred, target, mask)\n        loss.backward()\n        optimizer.step()\n\n        batch_size = imgs.size(0)\n        running_loss += loss.item() * batch_size\n        total_samples += batch_size\n\n        if (step + 1) % log_interval == 0:\n            avg_loss = running_loss / total_samples\n            print(f'Epoch {epoch} Step {step + 1}/{len(loader)} - Train Loss: {avg_loss:.4f}')\n\n    return running_loss / max(total_samples, 1)\n\n\n@torch.no_grad()\ndef evaluate(model: MAEViT, loader, device, config: Dict) -> Dict[str, float]:\n    model.eval()\n    total_loss = 0.0\n    total_metric = 0.0\n    total_samples = 0\n    mask_ratio = float(config['model']['mask_ratio'])\n\n    for imgs in loader:\n        imgs = imgs.to(device, non_blocking=True)\n        pred, target, mask = model(imgs, mask_ratio=mask_ratio)\n        loss = mae_loss(pred, target, mask)\n        metric = reconstruction_mse(pred, target, mask)\n\n        batch_size = imgs.size(0)\n        total_loss += loss.item() * batch_size\n        total_metric += metric.item() * batch_size\n        total_samples += batch_size\n\n    avg_loss = total_loss / max(total_samples, 1)\n    avg_metric = total_metric / max(total_samples, 1)\n    return {'loss': avg_loss, 'reconstruction_mse': avg_metric}\n\n\ndef main() -> None:\n    args = parse_args()\n    config = load_config(args.config, args.overrides)\n\n    seed = int(config.get('seed', 0))\n    seed_everything(seed)\n\n    device = get_device(config)\n\n    model = build_model(config).to(device)\n\n    tr_cfg = config['training']\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=float(tr_cfg['lr']),\n        weight_decay=float(tr_cfg['weight_decay']),\n    )\n\n    train_loader = get_dataloader(config, train=True)\n    val_loader = get_dataloader(config, train=False)\n\n    output_dir = config.get('output_dir', 'checkpoints')\n    ensure_dir(output_dir)\n\n    start_epoch = 1\n    best_val_loss = float('inf')\n\n    resume_path = tr_cfg.get('resume_path')\n    if resume_path:\n        if os.path.isfile(resume_path):\n            print(f'Resuming from checkpoint: {resume_path}')\n            ckpt = load_checkpoint(resume_path, model, optimizer, map_location=device)\n            start_epoch = int(ckpt.get('epoch', 0)) + 1\n            best_val_loss = float(ckpt.get('best_val_loss', best_val_loss))\n        else:\n            print(f'Warning: resume_path {resume_path} not found, starting from scratch.')\n\n    num_epochs = int(tr_cfg['epochs'])\n\n    for epoch in range(start_epoch, num_epochs + 1):\n        t0 = time.time()\n        train_loss = train_one_epoch(model, train_loader, optimizer, device, config, epoch)\n        val_stats = evaluate(model, val_loader, device, config)\n        dt = time.time() - t0\n\n        val_loss = val_stats['loss']\n        print(\n            f'Epoch {epoch}/{num_epochs} - ' \\\n            f'Train Loss: {train_loss:.4f} - ' \\\n            f'Val Loss: {val_loss:.4f} - ' \\\n            f'Val Recon MSE: {val_stats[\"reconstruction_mse\"]:.4f} - ' \\\n            f'Time: {dt:.1f}s'\n        )\n\n        ckpt = {\n            'epoch': epoch,\n            'model_state': model.state_dict(),\n            'optimizer_state': optimizer.state_dict(),\n            'config': config,\n            'best_val_loss': best_val_loss,\n        }\n        last_path = os.path.join(output_dir, 'last.pth')\n        save_checkpoint(ckpt, last_path)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_path = os.path.join(output_dir, 'best.pth')\n            save_checkpoint(ckpt, best_path)\n            print(f'New best model saved to {best_path}')\n\n\nif __name__ == '__main__':\n    main()\n"
    },
    {
      "path": "README.md",
      "content": "# MAE ViT Self-Supervised Training (PyTorch)\n\nMinimal, runnable implementation of a Masked Autoencoder (MAE) with a Vision Transformer (ViT) backbone, trained in a self-supervised way on a synthetic dataset.\n\n## Features\n\n- Pure PyTorch (no Lightning, no distributed training, no mixed precision)\n- MAE-style ViT encoder/decoder with patchify, random masking, and reconstruction\n- Simple synthetic image dataset (no external downloads required)\n- Config-driven hyperparameters via `config.yaml`\n- Command-line config overrides (e.g. `training.epochs=5`)\n- Deterministic seeding utilities\n- Training and evaluation loops\n- Checkpoint save/load (last and best)\n\n## Installation\n\n```bash\npip install -r requirements.txt\n```\n\n## Configuration\n\nThe default configuration is in `config.yaml`. Key sections:\n\n- `model`: ViT / MAE architecture (image size, patch size, dims, depths, mask ratio)\n- `training`: epochs, batch size, learning rate, device, etc.\n- `dataset`: synthetic dataset settings (image size, train/val sizes)\n- `seed`: global random seed\n- `output_dir`: where checkpoints are written\n\nExample snippet from `config.yaml`:\n\n```yaml\nmodel:\n  image_size: 32\n  patch_size: 4\n  in_channels: 3\n  embed_dim: 256\n  depth: 4\n  num_heads: 4\n  mlp_ratio: 4.0\n  decoder_embed_dim: 128\n  decoder_depth: 2\n  decoder_num_heads: 4\n  mask_ratio: 0.75\n```\n\n## Running Training\n\nBasic run using the default config:\n\n```bash\npython train.py\n```\n\nChoose CPU explicitly:\n\n```bash\npython train.py training.device=cpu\n```\n\nChange the number of epochs and batch size from the command line:\n\n```bash\npython train.py training.epochs=5 training.batch_size=128\n```\n\nAll overrides use `key=value` format, where `key` can be nested with dots, e.g. `model.mask_ratio=0.9`.\n\n## Checkpoints\n\nCheckpoints are saved in `output_dir` (default: `checkpoints/`):\n\n- `last.pth`: latest epoch\n- `best.pth`: best validation loss so far\n\nTo resume from a checkpoint, set in `config.yaml` or via CLI:\n\n```bash\npython train.py training.resume_path=checkpoints/last.pth\n```\n\n## Files\n\n- `config.yaml` \u2013 Configuration for model, training, dataset, and output\n- `model.py` \u2013 MAE ViT encoder/decoder implementation\n- `dataset.py` \u2013 Synthetic image dataset and dataloader factory\n- `train.py` \u2013 Training and evaluation loops, checkpointing, config handling\n- `losses.py` \u2013 MAE reconstruction loss on masked patches\n- `metrics.py` \u2013 Simple reconstruction metrics\n- `utils.py` \u2013 Seeding, checkpoint utilities, and config/CLI parsing\n- `requirements.txt` \u2013 Minimal Python dependencies\n\n## Notes\n\n- The provided synthetic dataset is purely random noise; this is intended to make the repo fully self-contained and runnable without downloads.\n- To adapt to a real dataset, extend `dataset.py` with your own dataset loader and point `dataset.name` to it.\n"
    },
    {
      "path": "requirements.txt",
      "content": "torch\npyyaml\nnumpy\n"
    }
  ]
}